/*
 *	Copyright 2023 Jan Pfeifer
 *
 *	Licensed under the Apache License, Version 2.0 (the "License");
 *	you may not use this file except in compliance with the License.
 *	You may obtain a copy of the License at
 *
 *	http://www.apache.org/licenses/LICENSE-2.0
 *
 *	Unless required by applicable law or agreed to in writing, software
 *	distributed under the License is distributed on an "AS IS" BASIS,
 *	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *	See the License for the specific language governing permissions and
 *	limitations under the License.
 */

// Package train holds tools to help run a training loop.
//
// It provides various levels of tooling, from supporting training
// one step, or a full loop. But it should serve also as an example
// that users that needs more flexibility can start from to create
// their own training loops.
package train

import (
	"fmt"
	"github.com/gomlx/gomlx/graph"
	"github.com/gomlx/gomlx/ml/context"
	"github.com/gomlx/gomlx/ml/train/metrics"
	"github.com/gomlx/gomlx/ml/train/optimizers"
	"github.com/gomlx/gomlx/types/tensor"
	"github.com/pkg/errors"
	"io"
)

const (
	// TrainerAbsoluteScope used for Context parameters related to the trainer.
	TrainerAbsoluteScope = context.ScopeSeparator + "trainer"

	// TrainerLossGraphParamKey is the key to the global params that holds the
	// losses added to the model.
	TrainerLossGraphParamKey = "trainer_loss"

	// TrainerPerStepUpdateGraphFnParamKey is used by AddPerStepUpdateGraphFn.
	TrainerPerStepUpdateGraphFnParamKey = "trainer_per_step_update_graph_fn"
)

// Trainer is a helper object to orchestrate a training step and evaluation.
//
// Given the inputs and labels, it deals with executing a training step (TrainStep) and
// evaluation (EvalStep and Eval), calling the loss and optimizer and running metrics.
//
// See Loop for a flexible and extensible (different UIs) way to run this in a training loop.
type Trainer struct {
	manager   *graph.Manager
	context   *context.Context
	deviceNum int
	modelFn   ModelFn
	lossFn    LossFn
	optimizer optimizers.Interface

	// maxExecutors to cache. It will fail after that. One executor is created
	// per info value, so this is the same as the max number of different info
	// values seen.
	maxExecutors              int
	inputsAndLabelsLenPerInfo map[any][2]int

	// Training data
	trainStepExecMap map[any]*context.Exec
	trainMetrics     []metrics.Interface

	// Eval data
	evalStepExecMap map[any]*context.Exec
	evalMetrics     []metrics.Interface
}

// ModelFn is a computation graph building function that takes as input a `spec` and a slice of `inputs`
// (even if just one) generated by a Dataset, and as output a slice (even if only one) of the `predictions`
// (or sometimes the logits).
//
// The `predictions` output by ModelFn is fed to a LossFn and to MetricsFn during training.
//
// Notice `spec` is opaque to train package, it's passed from the train.Dataset to the ModelFn, and its
// meaning is determined by the train.Dataset used. For static case (where data is always the same) it can simply be
// nil. Each value of `spec` is mapped to different computation graphs by the train.Trainer.
type ModelFn func(ctx *context.Context, spec any, inputs []*graph.Node) (predictions []*graph.Node)

// LossFn takes the output of ModelFn (called predictions, but it could be the logits),
// and the labels (coming out of Dataset.Yield()), and outputs the scalar loss, that can
// be used for training.
//
// For some types of self-supervised models for which there are no labels, the labels can be empty.
//
// Most of the predefined losses in package `gomlx/ml/train/losses` assume labels and predictions are
// both of length one. For multi-head models, it's very easy to write a small custom LossFn that splits
// the slice and send each label/prediction pair to a predefined loss.
type LossFn func(labels, predictions []*graph.Node) *graph.Node

// DefaultMaxExecutors used for Trainer objects. Each different `info` value from a Dataset triggers
// the creation of a new executor.
var DefaultMaxExecutors = 20

// NewTrainer constructs a trainer that can be used for training steps and evaluation. It also creates a new Context
// for model, which will hold the variables, hyperparameters and other information. It can be changed by the user.
//
// Its arguments are:
//
//   - manager needed to create and compile computation graphs.
//
//   - ctx (will) hold the variables, hyperparameters and related information for the model.
//
//   - modelFn builds the graph that transforms inputs into predictions (or logits).
//
//   - lossFn takes the predictions (the output of modelFn) and the labels and outputs the loss. If the
//     returned loss is not a scalar, it will be ReduceAllMean to a scalar.
//     There are several standard losses available in gomlx/ml/train/losses package. They can simply be used
//     as is, or called by arbitrary custom losses. It can also be set to nil, if it's doing
//     unsupervised training, or for any other reason if the model uses
//
//   - optimizer (e.g: optimizers.StochasticGradientDescent) is the methodology to improve the model variables (aka.
//     parameters or weights) to minimize the loss (the output of lossFn), typically using gradient descent.
//
//   - trainMetrics are output by trainer.TrainStep after each step. Here it's recommended to use moving average
//     types of metrics, since the model is changing so a mean wouldn't make sense. The mean loss of the batch and
//     a moving average of the loss is always included (the first two) by default. It's ok to be empty (nil).
//
//   - evalMetrics are output by trainer.EvalStep and trainer.Eval. Here it's recommend to use mean metrics, since the model
//     is presumably frozen, and it sees each example exactly once. The mean of the loss of the dataset is always provided
//     as the first metric. It's ok to be empty (nil).
func NewTrainer(manager *graph.Manager, ctx *context.Context,
	modelFn ModelFn, lossFn LossFn, optimizer optimizers.Interface,
	trainMetrics, evalMetrics []metrics.Interface) *Trainer {

	r := &Trainer{
		manager:   manager,
		context:   ctx,
		deviceNum: manager.DefaultDeviceNum(),
		modelFn:   modelFn,
		lossFn:    lossFn,
		optimizer: optimizer,

		maxExecutors:              DefaultMaxExecutors,
		inputsAndLabelsLenPerInfo: make(map[any][2]int),
		trainStepExecMap:          make(map[any]*context.Exec),
		evalStepExecMap:           make(map[any]*context.Exec),
	}

	// Create a context executor for TrainStep. Automatically include batch loss and moving average loss metrics.
	numMetrics := len(trainMetrics) + 2
	lossAndMetrics := make([]metrics.Interface, numMetrics)
	batchLossFn := func(_ *context.Context, labels, predictions []*graph.Node) *graph.Node {
		// Assume lossVar has already been set.
		g := predictions[0].Graph()
		loss := GetLosses(ctx, g)
		if loss == nil {
			return graph.ScalarZero(g, predictions[0].DType())
		}
		return loss
	}
	lossAndMetrics[0] = metrics.NewBaseMetric("Batch Loss", "loss", metrics.LossMetricType, batchLossFn, nil)
	lossAndMetrics[1] = metrics.NewExponentialMovingAverageMetric("Moving Average Loss", "~loss", metrics.LossMetricType, batchLossFn, nil, 0.01)
	copy(lossAndMetrics[2:], trainMetrics)
	r.trainMetrics = lossAndMetrics

	// Create a context executor for EvalStep. Automatically include mean loss metric as the first eval metric.
	numMetrics = len(evalMetrics) + 1
	lossAndMetrics = make([]metrics.Interface, numMetrics)
	lossAndMetrics[0] = metrics.NewMeanMetric("Mean Loss", "#loss", metrics.LossMetricType, batchLossFn, nil)
	copy(lossAndMetrics[1:], evalMetrics)
	r.evalMetrics = lossAndMetrics
	return r
}

// enumerateExec enumerates all executors maintained by the Trainer and call fn on them.
func (r *Trainer) enumerateExecs(fn func(exec *context.Exec)) {
	for _, exec := range r.trainStepExecMap {
		fn(exec)
	}
	for _, exec := range r.evalStepExecMap {
		fn(exec)
	}
}

// InDevice sets the device num to be used when executing graphs.
// TODO: Add support for training across multiple devices -- maybe a different Trainer for that, in principle should be simple.
// This should be called before any invocations of TrainStep.
// It returns a reference to itself so calls can be cascaded.
func (r *Trainer) InDevice(deviceNum int) *Trainer {
	r.deviceNum = deviceNum
	r.enumerateExecs(func(exec *context.Exec) {
		exec.InDevice(deviceNum)
	})
	return r
}

// Context returns the current Context. See SetContext to change it.
func (r *Trainer) Context() *context.Context {
	return r.context
}

// SetContext associates the given Context to the trainer. Should
// be called before any calls to Train or Evaluate.
// Notice that after the first time context is used to build a graph,
// it is set to Reuse. If the Context variables were already created,
// it should be marked with Context.Reuse.
// It returns a reference to itself so calls can be cascaded.
func (r *Trainer) SetContext(ctx *context.Context) *Trainer {
	r.context = ctx
	r.enumerateExecs(func(exec *context.Exec) {
		exec.SetContext(ctx)
	})
	return r
}

// TrainMetrics returns the train metrics objects (not the actual values just the objects
// that implement them).
func (r *Trainer) TrainMetrics() []metrics.Interface { return r.trainMetrics }

// EvalMetrics returns the eval metrics objects (not the actual values just the objects
// that implement them).
func (r *Trainer) EvalMetrics() []metrics.Interface { return r.evalMetrics }

// createExecutor (train or eval) for the given info. Returns an error if it failed for
// any reason, including exceeding maxExecutors.
func (r *Trainer) createExecutor(spec any, inputsLen, labelsLen int,
	graphFn func(spec any, ctx *context.Context, inputs, labels []*graph.Node) (metrics []*graph.Node)) (*context.Exec, error) {
	numExecs := len(r.trainStepExecMap) + len(r.evalStepExecMap)
	if numExecs > r.maxExecutors {
		return nil, errors.Errorf("Max number of executors reached: one is created for each "+
			"different value of `spec` returned by Dataset, triggering a different JIT-compiled "+
			"computation graph. Probably you want to limit the number of different datasets configuration "+
			"(spec) supported, or increase train.DefaultMaxExecutor if this is what you want. Value of spec "+
			"passed at this interation: %+v", spec)
	}
	if numExecs > 0 {
		r.context = r.context.Checked(false) // Only check for duplicate variables at the first graph creation.
	}
	r.inputsAndLabelsLenPerInfo[spec] = [2]int{inputsLen, labelsLen}
	trainerName := "Trainer"
	if _, found := spec.(fmt.Stringer); found {
		trainerName = fmt.Sprintf("Trainer: spec=%s", spec)
	}
	exec := context.NewExec(r.manager, r.context,
		func(ctx *context.Context, inputsAndLabels []*graph.Node) (metrics []*graph.Node) {
			inputs := inputsAndLabels[:inputsLen]
			labels := inputsAndLabels[inputsLen:]
			return graphFn(spec, ctx, inputs, labels)
		}).WithName(trainerName)
	if exec == nil {
		return nil, errors.Errorf("Failed to create a computation graph for spec=%+v", spec)
	}
	return exec, nil
}

// trainStepGraph builds the graph to train one step. It is called by the context executor (`r.trainStepExecMap`)
// everytime a graph needs to be built (typically for new batch sizes).
// inputsAndLabel[:-1] are the inputs, and inputsAndLabel[-1] is the labels batch.
// It a slice with the loss and the updates created by the optimizer.
func (r *Trainer) trainStepGraph(spec any, ctx *context.Context, inputs, labels []*graph.Node) (metrics []*graph.Node) {
	g := inputs[0].Graph()
	if !g.Ok() {
		return nil
	}
	ctx.SetTraining(g, true) // Some layers behave differently if in training.

	// AddLoss generated by the given lossFn.
	predictions := r.modelFn(ctx, spec, inputs)
	if !g.Ok() || len(predictions) == 0 {
		return nil
	}
	if r.lossFn != nil {
		loss := r.lossFn(labels, predictions)
		if !loss.Shape().IsScalar() {
			loss = graph.ReduceAllMean(loss)
		}
		if !g.Ok() || !ctx.Ok() {
			return nil
		}
		AddLoss(ctx, loss)
	}

	// Store total loss as a variable, so it can be used by metrics.
	loss := GetLosses(ctx, g)
	if loss == nil {
		g.SetErrorf("no loss function defined (or it returned nil), and no loss set with AddLoss(), there is nothing to optimize!?")
		return nil
	}

	// Optimizer: it will create graph for gradient.
	r.optimizer.UpdateGraph(ctx, g, loss)
	if !g.Ok() || !ctx.Ok() {
		return nil
	}

	// Execute registered ContextGraphFn hooks for current graph.
	ctx.EnumerateGraphParams(g, func(scope string, key string, value any) {
		if !g.Ok() || !ctx.Ok() {
			return
		}
		if key != TrainerPerStepUpdateGraphFnParamKey {
			return
		}
		if fn, ok := value.(ContextGraphFn); ok {
			fn(ctx, g)
		}
	})
	if !g.Ok() || !ctx.Ok() {
		return nil
	}

	// Generate all metrics, which includes: batch loss, exponential moving average of the batch loss.
	metrics = r.metricsUpdatesGraph(ctx, labels, predictions, r.trainMetrics)
	if !g.Ok() {
		return nil
	}

	// Append model updates to the end of metricsUpdates
	return
}

// callGraphFn for TrainStep or EvalStep makes sure the builds the arguments for the Call() method, plus
// do standard checks on inputs and labels.
func (r *Trainer) callGraphFn(
	graphFn func(spec any, ctx *context.Context, inputs, labels []*graph.Node) (metrics []*graph.Node),
	execsMap map[any]*context.Exec,
	spec any, inputs, labels []tensor.Tensor) (metrics []tensor.Tensor, err error) {
	if len(inputs) == 0 {
		return nil, errors.Errorf("there are no inputs, at least one is required")
	}
	if lengths, found := r.inputsAndLabelsLenPerInfo[spec]; found {
		if len(inputs) != lengths[0] || len(labels) != lengths[1] {
			err = errors.Errorf("dataset yields inputs (%d) and labels (%d) with lengths different "+
				"than with previous call (%d and %d) for the given spec %+v", len(inputs), len(labels),
				lengths[0], lengths[1], spec)
			return
		}
	}
	for ii, input := range inputs {
		if input == nil {
			err = errors.Errorf("inputs[%d] is nil!?", ii)
			return
		}
		if input.Error() != nil {
			err = errors.Wrapf(input.Error(), "invalid inputs[%d], with error %v", ii, input.Error())
			return
		}
	}

	// Create arguments as []any and run trainStepExecMap.Call().
	numParams := len(inputs) + len(labels)
	inputsAndLabels := make([]any, 0, numParams)
	for _, t := range inputs {
		inputsAndLabels = append(inputsAndLabels, t)
	}
	for _, t := range labels {
		inputsAndLabels = append(inputsAndLabels, t)
	}

	// Get executor.
	exec, found := execsMap[spec]
	if !found {
		exec, err = r.createExecutor(spec, len(inputs), len(labels), graphFn)
		if err != nil {
			return
		}
		execsMap[spec] = exec
	}

	// Run trainStepExecMap and check everything went fine.
	metrics, err = exec.Call(inputsAndLabels...)
	if err != nil {
		err = errors.WithMessage(err, "failed to execute train/eval step")
		return
	}
	if len(metrics) == 0 {
		err = errors.Errorf("no metrics calculate metric in step")
		return
	}
	return
}

// metricsUpdatesGraph creates the graph for a set of metrics.
func (r *Trainer) metricsUpdatesGraph(ctx *context.Context, labels, predictions []*graph.Node,
	metricsObjects []metrics.Interface) (metrics []*graph.Node) {
	g := predictions[0].Graph()
	if !g.Ok() {
		return nil
	}
	numMetrics := len(metricsObjects)
	metrics = make([]*graph.Node, 0, numMetrics)
	ctxUnchecked := ctx.Checked(false)
	for _, metric := range metricsObjects {
		metricResult := metric.UpdateGraph(ctxUnchecked, labels, predictions)
		metrics = append(metrics, metricResult)
	}
	return
}

// TrainStep runs one step and returns the metrics or an error.
//
// All arguments usually come from `Dataset.Yield`, see a more detailed description there. In short:
//
//   - info: provided by the dataset. Often just nil. Each value will trigger the creation
//     of different computation graphs. Normally static values (for the dataset) used to describe
//     the inputs. See longer discussion in `train.Dataset`.
//   - inputs: always a slice, even though it's common to have only one input tensor in the slice.
//     There must be always at least one input. For each `info` value, the number of inputs and labels
//     must remain constant. It will return an error otherwise.
//   - labels: also always a slice, even if commonly with only one tensor.
//
// It returns a slice of metrics, that includes (the first two) the batch loss, and the moving exponential average
// of the batch loss, plus the other `trainMetrics` configured during the creation onf the Trainer.
func (r *Trainer) TrainStep(spec any, inputs, labels []tensor.Tensor) (metrics []tensor.Tensor, err error) {
	return r.callGraphFn(r.trainStepGraph, r.trainStepExecMap, spec, inputs, labels)
}

// evalStepGraph builds the graph to eval one step. It is called by the context executor (`r.evalStepExecMap`)
// everytime a graph needs to be built (typically for new batch sizes).
// inputsAndLabel[:-1] are the inputs, and inputsAndLabel[-1] is the labels batch.
func (r *Trainer) evalStepGraph(spec any, ctx *context.Context, inputs, labels []*graph.Node) (metrics []*graph.Node) {
	g := inputs[0].Graph()
	if !g.Ok() {
		return nil
	}
	ctx.SetTraining(g, false) // Some layers behave differently in train/eval.

	predictions := r.modelFn(ctx, spec, inputs)
	if r.lossFn != nil {
		loss := r.lossFn(labels, predictions)
		if !loss.Shape().IsScalar() {
			loss = graph.ReduceAllMean(loss)
		}
		if !g.Ok() || !ctx.Ok() {
			return nil
		}
		AddLoss(ctx, loss)
	}

	// Get metrics and updates
	metrics = r.metricsUpdatesGraph(ctx, labels, predictions, r.evalMetrics)
	if !g.Ok() {
		return nil
	}
	return
}

// ResetTrainMetrics call Metrics.Reset on all train metrics. Usually called before a training session.
func (r *Trainer) ResetTrainMetrics() error {
	for _, metric := range r.trainMetrics {
		err := metric.Reset(r.context.Checked(false))
		if err != nil {
			return errors.WithMessagef(err, "Eval() failed to reset metric %q", metric.Name())
		}
	}
	return nil
}

// EvalStep runs one eval step and returns the metrics, the first one being the mean loss, or an error.
//
// The parameters are the output of a Dataset.Yield call. The same as TrainStep.
//
// It returns the current value for the registered eval metrics.
func (r *Trainer) EvalStep(spec any, inputs, labels []tensor.Tensor) (metrics []tensor.Tensor, err error) {
	return r.callGraphFn(r.evalStepGraph, r.evalStepExecMap, spec, inputs, labels)
}

// resetEvalMetrics call Metrics.Reset on all eval metrics.
func (r *Trainer) resetEvalMetrics() error {
	for _, metric := range r.evalMetrics {
		err := metric.Reset(r.context)
		if err != nil {
			return errors.WithMessagef(err, "Eval() failed to reset metric %q", metric.Name())
		}
	}
	return nil
}

// Eval returns the computation of loss and metrics over the given dataset. The dataset
// has to be finite (yield io.EOF at the end). The function will reset the dataset
// at the start.
func (r *Trainer) Eval(ds Dataset) (lossAndMetrics []tensor.Tensor, err error) {
	ds.Reset()
	if err = r.resetEvalMetrics(); err != nil {
		return nil, err
	}
	count := 0
	for {
		spec, inputs, labels, err := ds.Yield()
		if err == io.EOF {
			break
		}
		count++
		if err != nil {
			return nil, err
		}
		// Early free (not wait for the GC) of the results of previous batch.
		for _, t := range lossAndMetrics {
			t.FinalizeAll()
		}
		lossAndMetrics, err = r.EvalStep(spec, inputs, labels)
		if err != nil {
			return nil, err
		}
	}
	if count == 0 {
		return nil, errors.Errorf("evaluation dataset yielded no batches, no data to evaluate")
	}
	return lossAndMetrics, nil
}

// Metrics return list of registered eval metrics, including the loss metric that is added automatically.
func (r *Trainer) Metrics() []metrics.Interface {
	return r.evalMetrics
}

// GetTrainExec returns the "train step" executor (context.Exec). This accessor allows tools (like
// collector.DataCollector) access to the executor to collect information for debugging, plotting, etc.
func (r *Trainer) GetTrainExec(spec any) *context.Exec {
	return r.trainStepExecMap[spec]
}

// GetEvalExec returns the "eval step" executor (context.Exec). This accessor allows tools (like
// collector.DataCollector) access to the executor to collect information for debugging, plotting, etc.
func (r *Trainer) GetEvalExec(spec any) *context.Exec {
	return r.evalStepExecMap[spec]
}

// AddLoss adds the given scalar loss (if it is not scalar, it will be reduced with ReduceAllMean)
// to the context's Params. This is the loss used by the trainer to optimize the model.
//
// The loss is added to previous values of loss added (if any), they should all have the same dtype.
func AddLoss(ctx *context.Context, loss *graph.Node) {
	g := loss.Graph()
	if !g.Ok() {
		return
	}
	ctxTrainer := ctx.InAbsPath(TrainerAbsoluteScope)
	if !loss.Shape().IsScalar() {
		loss = graph.ReduceAllMean(loss)
	}

	currentLoss, found := ctxTrainer.GetGraphParam(g, TrainerLossGraphParamKey)
	if found {
		loss = graph.Add(loss, currentLoss.(*graph.Node))
	}
	ctxTrainer.SetGraphParam(g, TrainerLossGraphParamKey, loss)
}

// GetLosses returns the sum of all loss terms added with AddLoss(), or nil if none was set.
//
// Usually this is used by the trainer after all losses are accounted for. But can be used
// by arbitrary modeling functions. In particular after the optimizer update, see AddPerStepUpdateGraphFn.
func GetLosses(ctx *context.Context, g *graph.Graph) (loss *graph.Node) {
	ctxTrainer := ctx.InAbsPath(TrainerAbsoluteScope)
	lossAny, _ := ctxTrainer.GetGraphParam(g, TrainerLossGraphParamKey)
	return lossAny.(*graph.Node)
}

// ContextGraphFn is a generic graph building function.
type ContextGraphFn func(ctx *context.Context, g *graph.Graph)

// AddPerStepUpdateGraphFn registers the given function to be executed at every training step, after optimizer
// updates the variables with the gradient.
//
// This allows one for instance to implement variable constraints.
//
// There can be one ContextGraphFn registered per scope per graph. If one wants to register more than one
// such functions, use a different context scopes.
//
// One thing to observe: this is executed after the optimizer and therefore also after the loss is calculated.
// Any changes made to the model weights won't be reflected on the loss returned by the training step. Nor most
// of the metrics: the metrics are updated after this hook, but they typically use the predictions that
// were also generated earlier in the training.
func AddPerStepUpdateGraphFn(ctx *context.Context, g *graph.Graph, fn ContextGraphFn) {
	ctx.SetGraphParam(g, TrainerPerStepUpdateGraphFnParamKey, fn)
}
